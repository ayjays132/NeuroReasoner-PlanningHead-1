{
  "architectures": [
    "NeuroReasonerPlanningHead1"
  ],
  "attention_probs_dropout_prob": 0.1,
  "auto_map": {
    "AutoConfig": "modeling_planning_head.NeuroReasonerPlanningHead1Config",
    "AutoModel": "modeling_planning_head.NeuroReasonerPlanningHead1",
    "AutoModelForCausalLM": "modeling_planning_head.NeuroReasonerPlanningHead1"
  },
  "base_model_name_or_path": "ayjays132/NeuroReasoner-1-NR-1",
  "model_name": "NeuroReasoner-PlanningHead-1",
  "bos_token_id": 50256,
  "eos_token_id": 50256,
  "experiment_weight": 0.3,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "inv_forge_loss_weight": 0.5,
  "invariant_weight": 0.2,
  "iterative_loss_weight": 0.5,
  "iterative_max_iterations": 5,
  "iterative_refine_weight": 0.3,
  "lab_head_loss_weight": 0.5,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 1024,
  "memory_dropout": 0.1,
  "memory_heads": 4,
  "memory_loss_weight": 0.1,
  "memory_size": 64,
  "model_type": "neuroreasoner_planninghead",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "plan_dim": 128,
  "plan_dropout": 0.1,
  "plan_loss_weight": 1.0,
  "plan_num_layers": 2,
  "schema_bridge_loss_weight": 0.5,
  "schema_weight": 0.3,
  "torch_dtype": "float32",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_invariant_forge": true,
  "use_lab_head": true,
  "use_memory": true,
  "use_schema_bridge": true,
  "vocab_size": 50308
}
